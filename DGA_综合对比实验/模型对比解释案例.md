## DGA 域名判定多模型对比解释案例

本文记录同一个可疑域名在三类模型（XGBoost、RandomForest、LSTM 序列模型）下的解释性输出，并分析它们利用信息的差异、互补性与局限。该案例有助于教学展示“特征工程模型 vs 端到端序列模型”在可解释维度上的不同视角。

### 目标域名

```
1df5hr42x3s651dgh56tdbq6bs.org
```

域名主体（去掉后缀 .org）长度 26，混合字母与数字、缺少元音和可读片段，典型 DGA 形态。

---
### 1. XGBoost 特征贡献（启发式排序）

```
Feature                            Value       Imp   Value*Imp
length                           26.0000    0.5929     15.4165
unique_char_count                16.0000    0.1459      2.3351
entropy                           3.8441    0.2467      0.9482
digit_count                      11.0000    0.0044      0.0486
repeated_char_count              17.0000    0.0022      0.0381
bigram_avg_logp                  -8.2344    0.0037     -0.0305
max_consecutive_digits            3.0000    0.0014      0.0043
max_consecutive_consonants        4.0000    0.0002      0.0007
digit_ratio                       0.4231    0.0011      0.0005
vowel_count                       0.0000    0.0008      0.0000
vowel_ratio                       0.0000    0.0002      0.0000
dict_coverage                     0.0000    0.0005      0.0000
```

要点：
- XGBoost 的 importance 高度集中在少数 3~4 个特征：`length`、`entropy`、`unique_char_count`（以及部分 entropy）。
- 由于 boosting 贪心拟合残差，当核心少量特征足以高度区分时，其它相关特征（digit_ratio、bigram_avg_logp）被边缘化，importance 极低。
- `Value*Imp` 只是“值 × 全局重要度”的启发式，用来排序直观性，不代表真实可加贡献。

### 2. RandomForest 特征贡献（启发式排序）

```
Feature                            Value       Imp   Value*Imp
length                           26.0000    0.2900      7.5406
unique_char_count                16.0000    0.2320      3.7114
digit_count                      11.0000    0.1022      1.1246
entropy                           3.8441    0.1557      0.5985
bigram_avg_logp                  -8.2344    0.0684     -0.5635
max_consecutive_digits            3.0000    0.0503      0.1508
repeated_char_count              17.0000    0.0077      0.1302
digit_ratio                       0.4231    0.0803      0.0340
max_consecutive_consonants        4.0000    0.0003      0.0011
vowel_count                       0.0000    0.0001      0.0000
vowel_ratio                       0.0000    0.0127      0.0000
dict_coverage                     0.0000    0.0004      0.0000
```

对比观察：
- RF importance 更“分散”，`digit_count`、`digit_ratio`、`bigram_avg_logp`、`max_consecutive_digits` 拿到实质性权重。原因是 bagging + 随机特征子采样迫使森林在缺失主特征时使用替代信号。 
- 与 XGBoost 相比，RF 给“数字模式”与“二元文法概率”更多比重，显示它在多样随机子集中稳定出现。
- 仍有同一主线：长度 + 多样性 + 随机性（熵 / bigram 低概率 / 缺少元音）。

### 3. 端到端 LSTM 逐字符梯度显著性 & 隐状态范数

```
Predicted p(dga)=0.9999
Idx Ch  Grad(L2)    HiddenNorm(相对)
00 1  grad:0.1072 (最高)  hid:8.5909
01 d  grad:0.0152         hid:8.0604
02 f  grad:0.0099         hid:7.9113
...  (中间若干略)
20 d  grad:0.0128         hid:7.1813
21 b  grad:0.0117         hid:7.6451
22 q  grad:0.0112         hid:7.4737
23 6  grad:0.0116         hid:8.0013
24 b  grad:0.0159         hid:8.3544
25 s  grad:0.0146         hid:7.7003
```

解读：
- 首字符 `1` 梯度最大，说明其嵌入向量对最终 logit 的敏感度最高 —— 序列起点特征（数字开头 + 后续随机结构）对模型早期判定影响大。
- 隐状态范数在中后段维持较高，表示模型已经“锁定”类别；后面字符提供微调而非根本逆转。
- 尾部一段字符再次出现较集中的中等梯度，说明模型对结尾局部模式仍有关注（混合字母数字延续随机性）。
- 高梯度 ≠ 高 hidden norm：梯度衡量“局部扰动敏感度”，范数反映“当前信息累积强度”。

### 4. 三种视角的信息利用差异

| 视角 | 主要证据信号 | 行为模式 | 风险 / 局限 |
| ---- | ------------- | -------- | ----------- |
| XGBoost | length + unique_char_count + entropy | 少数主特征快速拟合 | 对特征删减/对抗扰动潜在更敏感（特征集中） |
| RandomForest | length + diversity + (digit_count/ratio) + bigram_avg_logp + 随机性补充 | 多信号冗余累积 | 解释分散、局部精确贡献不直观 |
| LSTM | 原始字符序列全局与局部模式 | 早期捕获 + 后续巩固 | 梯度为局部线性近似，原始分值可受饱和 & 噪声影响 |

### 5. 为什么 RF 会更看重数字/二元文法特征
1. Bagging 与特征随机子集使得某些树缺失长度/熵时，数字与 bigram 统计成为替代分裂点，从而提高全局出现频率。
2. 数据中“字母数字交错 + 缺少元音”与 DGA 高相关，RF 将多个弱相关特征累积提高稳定性。
3. Boosting 更倾向压缩冗余特征；RF 鼓励冗余信号保留，形成鲁棒性冗余。

### 6. LSTM 与手工特征的“对齐”映射（概念性）
| 手工特征 | LSTM 内隐学习方式 | 指标迹象 |
| -------- | ---------------- | -------- |
| length | 序列时间步数量 | 中后段 hidden norm 持续稳定升高 |
| unique_char_count / entropy | 字符 embedding 分布多样性在递归叠加 | hidden 表征能量积累，梯度分散 |
| digit_ratio / max_consecutive_digits | 数字嵌入模式的连续出现 | 局部 time-step 激活模式重复 |
| bigram_avg_logp | 低频组合序列化统计 | LSTM 捕获不可读 n-gram，梯度不集中于可读块 |
| dict_coverage / vowel_ratio | 缺少可读片段/元音 | 模型缺乏对“词样子”模式的强化激活（无特定脉冲） |

### 7. “Value*Imp” 的注意事项
`Value*Imp` ≠ 局部可加贡献：
- 真实可加解释需使用 SHAP（TreeSHAP）或 LIME。
- 当前列仅用于对比排序，给出“该样本上哪些特征同时数值较极端又有较高全局权重”。
- 不适合相加重构 logit / 概率。

### 8. 何时更偏向用哪种模型？
| 场景 | 推荐模型 | 理由 |
| ---- | -------- | ---- |
| 快速原型、少量特征即可高区分 | XGBoost | 训练快、结构紧凑 | 
| 需要一定鲁棒性 + 避免过度依赖单一特征 | RandomForest | 信号冗余 & 抗噪声 |
| 无法提前定义特征 / 希望端到端扩展到子域、URL 级别 | LSTM / 深度序列 | 自适应表示能力 |
| 需要严格可加解释（审计） | 树模型 + TreeSHAP | 数学保证的一致性 |
| 想做字符级攻击/扰动实验 | LSTM | 直接操作序列空间 |

### 9. 后续可扩展的解释增强
1. 集成梯度 (Integrated Gradients)：减少局部噪声，获得更稳健 per-token 贡献。
2. SmoothGrad：多次加噪平均，平滑梯度显著性。
3. TreeSHAP：为 RF / XGB 输出真正可加的 per-feature 贡献分解。
4. JSON 导出：整合 classical + 序列 saliency，便于前端热力图可视化。
5. 对抗/扰动分析：系统评估修改首字符/局部数字串对三模型置信度的影响，衡量脆弱点。

### 10. 结论摘要
该域名被三种模型一致高置信度判定为 DGA。差异体现在：
- XGBoost 高度集中少量强特征。
- RandomForest 利用多维冗余信号以增强稳定性。
- LSTM 利用端到端字符序列，表现为早期强触发 + 后续状态巩固，梯度揭示关键起始与尾部模式。

多视角联合解释增加了审计透明度：如果某类对抗样本削弱长度或字符多样性，树模型置信度可能下降，而 LSTM 仍可能从细粒度 n-gram 随机性中保持判定；反之若对字符序列进行微调规整，序列模型或首先受影响，而统计特征模型仍捕捉总体结构。

---
**附：术语速览**
- Importance: 模型全局特征重要度（基于分裂增益或出现频率的归一化）。
- Saliency(梯度显著性): 输出对输入嵌入的局部敏感度近似。
- Hidden State Norm: 每步隐藏向量的 L2 范数，表示累积表征能量，不等价于重要度。
- TreeSHAP: 提供一致性与可加性的树模型特征贡献方法。

